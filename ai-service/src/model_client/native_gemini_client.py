"""
Native Gemini Client for AutoGen compatibility
æ¡¥æ¥AutoGençš„OpenAIæ ¼å¼å’ŒGoogle GeminiåŸç”ŸAPI
"""

import asyncio
import json
import httpx
from typing import List, Dict, Any, Optional, AsyncGenerator, Union
from dataclasses import dataclass
from src.settings import settings
import logging
from autogen_core.models import CreateResult, UserMessage, AssistantMessage
from autogen_core._types import FunctionCall

logger = logging.getLogger(__name__)

# AutoGen native types are used instead of custom classes

class NativeGeminiClient:
    """
    åŸç”ŸGeminiå®¢æˆ·ç«¯ï¼Œå…¼å®¹AutoGençš„OpenAIæ¥å£
    """

    def __init__(self, model: str = "gemini-2.5-pro", api_key: Optional[str] = None):
        self.model = model
        self.api_key = api_key or settings.GEMINI_API_KEY
        self.base_url = "https://generativelanguage.googleapis.com/v1"
        self.client = httpx.AsyncClient(timeout=60.0)

        # AutoGenå…¼å®¹æ€§å±æ€§
        self.model_info = {
            "vision": False,
            "function_calling": True,
            "json_output": True,
            "structured_output": True,
            "multiple_system_messages": True,
        }

    async def create(
        self,
        messages: List[Dict[str, Any]],
        model: Optional[str] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[str] = None,
        stream: bool = False,
        **kwargs
    ) -> CreateResult:
        """
        AutoGenå…¼å®¹çš„createæ–¹æ³•
        """
        try:
            logger.info(f"ğŸ¯ Native Gemini Client - Creating completion for model: {model or self.model}")
            logger.info(f"ğŸ“ Messages count: {len(messages)}")
            logger.info(f"ğŸ”§ Tools count: {len(tools) if tools else 0}")

            # è½¬æ¢æ ¼å¼
            gemini_request = self._convert_to_gemini_format(messages, tools)
            logger.info(f"ğŸ“¤ Converted to Gemini format: {json.dumps(gemini_request, indent=2)}")

            # è°ƒç”¨Gemini API
            response_data = await self._call_gemini_api(gemini_request, model or self.model)
            logger.info(f"ğŸ“¥ Gemini API response: {json.dumps(response_data, indent=2, ensure_ascii=False)}")

            # è½¬æ¢å›AutoGenæ ¼å¼
            autogen_response = self._convert_to_autogen_format(response_data, tools)
            logger.info(f"âœ… Converted to AutoGen format")

            return autogen_response

        except Exception as e:
            logger.error(f"âŒ Native Gemini Client error: {e}")
            raise

    async def create_stream(
        self,
        messages: List[Dict[str, Any]],
        model: Optional[str] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[str] = None,
        **kwargs
    ) -> AsyncGenerator[CreateResult, None]:
        """
        AutoGenå…¼å®¹çš„create_streamæ–¹æ³• - æµå¼å“åº”
        ç›®å‰ç®€åŒ–å®ç°ï¼šè°ƒç”¨éæµå¼APIå¹¶ä½œä¸ºå•ä¸ªchunkè¿”å›
        """
        try:
            logger.info(f"ğŸ¯ Native Gemini Client - Creating streaming completion for model: {model or self.model}")

            # è°ƒç”¨éæµå¼createæ–¹æ³•
            result = await self.create(
                messages=messages,
                model=model,
                tools=tools,
                tool_choice=tool_choice,
                stream=False,
                **kwargs
            )

            # ä½œä¸ºå•ä¸ªchunkè¿”å›
            yield result

        except Exception as e:
            logger.error(f"âŒ Native Gemini Client streaming error: {e}")
            raise

    def _convert_to_gemini_format(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """
        å°†OpenAIæ ¼å¼è½¬æ¢ä¸ºGeminiæ ¼å¼
        """
        contents = []

        for message in messages:
            # å¤„ç†AutoGençš„æ¶ˆæ¯å¯¹è±¡å’Œå­—å…¸ä¸¤ç§æ ¼å¼
            logger.info(f"ğŸ” Message type: {type(message)}, message: {message}")

            # ä½¿ç”¨try-exceptæ›´å®‰å…¨åœ°å¤„ç†ä¸åŒæ ¼å¼
            try:
                # å…ˆå°è¯•å­—å…¸æ ¼å¼
                if hasattr(message, 'get') and callable(getattr(message, 'get')):
                    # æ™®é€šå­—å…¸æ ¼å¼
                    logger.info(f"âœ… Using dict format")
                    role = message.get("role", "")
                    content = message.get("content", "")
                else:
                    # AutoGenæ¶ˆæ¯å¯¹è±¡ï¼ˆSystemMessage, UserMessage, AssistantMessageç­‰ï¼‰
                    logger.info(f"âœ… Using AutoGen message object format")
                    # AutoGenå¯¹è±¡ä½¿ç”¨typeå±æ€§è€Œä¸æ˜¯role
                    message_type = getattr(message, 'type', '')
                    content = getattr(message, 'content', '')

                    # å°†AutoGençš„typeæ˜ å°„åˆ°role
                    if message_type == 'SystemMessage':
                        role = 'system'
                    elif message_type == 'UserMessage':
                        role = 'user'
                    elif message_type == 'AssistantMessage':
                        role = 'assistant'
                    else:
                        # å°è¯•ä»sourceå±æ€§æ¨æ–­è§’è‰²
                        source = getattr(message, 'source', '')
                        if source == 'user':
                            role = 'user'
                        else:
                            role = 'assistant'
            except Exception as e:
                logger.error(f"âŒ Error processing message {message}: {e}")
                # å›é€€åˆ°å¯¹è±¡å±æ€§è®¿é—®
                message_type = getattr(message, 'type', '')
                content = getattr(message, 'content', '')

                # å°†AutoGençš„typeæ˜ å°„åˆ°role
                if message_type == 'SystemMessage':
                    role = 'system'
                elif message_type == 'UserMessage':
                    role = 'user'
                elif message_type == 'AssistantMessage':
                    role = 'assistant'
                else:
                    # å°è¯•ä»sourceå±æ€§æ¨æ–­è§’è‰²
                    source = getattr(message, 'source', '')
                    if source == 'user':
                        role = 'user'
                    else:
                        role = 'assistant'

            if role == "system":
                # ç³»ç»Ÿæ¶ˆæ¯ä½œä¸ºç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯çš„å‰ç¼€
                if contents and contents[-1].get("role") == "user":
                    contents[-1]["parts"][0]["text"] = f"{content}\n\n{contents[-1]['parts'][0]['text']}"
                else:
                    contents.append({
                        "role": "user",
                        "parts": [{"text": content}]
                    })
            elif role == "user":
                contents.append({
                    "role": "user",
                    "parts": [{"text": content}]
                })
            elif role == "assistant":
                contents.append({
                    "role": "model",
                    "parts": [{"text": content}]
                })

        request_data = {"contents": contents}

        # æš‚æ—¶å®Œå…¨è·³è¿‡å·¥å…·è°ƒç”¨å¤„ç†ï¼Œåªå‘é€æ¶ˆæ¯å†…å®¹
        # TODO: ä¿®å¤Gemini APIå·¥å…·å…¼å®¹æ€§é—®é¢˜
        if tools:
            logger.info(f"âš ï¸ Completely skipping tools processing due to API compatibility issues. Found {len(tools)} tools.")
        # æ³¨é‡Šæ‰æ‰€æœ‰å·¥å…·å¤„ç†é€»è¾‘
        # if tools:
        #     function_declarations = []
        #     for tool in tools:
        #         logger.info(f"ğŸ”§ Tool type: {type(tool)}, tool: {tool}")
        #         try:
        #             # å¤„ç†FunctionToolå¯¹è±¡å’Œå­—å…¸ä¸¤ç§æ ¼å¼
        #             if hasattr(tool, 'get') and callable(getattr(tool, 'get')):
        #                 # å­—å…¸æ ¼å¼
        #                 if tool.get("type") == "function":
        #                     func = tool["function"]
        #                     function_declarations.append({
        #                         "name": func["name"],
        #                         "description": func["description"],
        #                         "parameters": func.get("parameters", {})
        #                     })
        #             else:
        #                 # FunctionToolå¯¹è±¡æ ¼å¼
        #                 if hasattr(tool, 'function_schema'):
        #                     schema = tool.function_schema
        #                     function_declarations.append({
        #                         "name": schema.get("name", ""),
        #                         "description": schema.get("description", ""),
        #                         "parameters": schema.get("parameters", {})
        #                     })
        #                 elif hasattr(tool, 'name') and hasattr(tool, 'description'):
        #                     # ç›´æ¥ä»å¯¹è±¡å±æ€§è·å–
        #                     parameters = getattr(tool, 'parameters', {})
        #                     # ç¡®ä¿parametersæœ‰æ­£ç¡®çš„æ ¼å¼
        #                     if not parameters or not isinstance(parameters, dict):
        #                         parameters = {"type": "object", "properties": {}, "required": []}
        #                     function_declarations.append({
        #                         "name": getattr(tool, 'name', ''),
        #                         "description": getattr(tool, 'description', ''),
        #                         "parameters": parameters
        #                     })
        #         except Exception as e:
        #             logger.error(f"âŒ Error processing tool {tool}: {e}")
        #             continue
        #     if function_declarations:
        #         request_data["tools"] = [{"functionDeclarations": function_declarations}]

        return request_data

    async def _call_gemini_api(self, request_data: Dict[str, Any], model: str) -> Dict[str, Any]:
        """
        è°ƒç”¨GeminiåŸç”ŸAPI
        """
        url = f"{self.base_url}/models/{model}:generateContent"

        params = {"key": self.api_key}
        headers = {"Content-Type": "application/json"}

        logger.info(f"ğŸŒ Calling Gemini API: {url}")
        logger.info(f"ğŸ“¤ Request data: {json.dumps(request_data, indent=2, ensure_ascii=False)}")

        response = await self.client.post(
            url,
            json=request_data,
            params=params,
            headers=headers
        )

        if response.status_code != 200:
            error_text = response.text
            logger.error(f"âŒ Gemini API error {response.status_code}: {error_text}")
            raise Exception(f"Gemini API error {response.status_code}: {error_text}")

        return response.json()

    def _convert_to_autogen_format(self, gemini_response: Dict[str, Any], tools: Optional[List[Dict[str, Any]]] = None) -> CreateResult:
        """
        å°†Geminiå“åº”è½¬æ¢ä¸ºAutoGen CreateResultæ ¼å¼
        """
        candidates = gemini_response.get("candidates", [])
        if not candidates:
            raise Exception("No candidates in Gemini response")

        candidate = candidates[0]
        content = candidate.get("content", {})
        parts = content.get("parts", [])

        # æå–æ–‡æœ¬å†…å®¹
        message_content = ""
        function_calls = []

        for part in parts:
            if "text" in part:
                message_content += part["text"]
            elif "functionCall" in part:
                # å¤„ç†å·¥å…·è°ƒç”¨ - ä½¿ç”¨FunctionCallè€Œä¸æ˜¯FunctionExecutionResult
                func_call = part["functionCall"]
                function_calls.append(FunctionCall(
                    id=f"call_{len(function_calls)}",
                    name=func_call.get("name", ""),
                    arguments=json.dumps(func_call.get("args", {}))
                ))

        # æ„å»ºä½¿ç”¨ç»Ÿè®¡
        usage_metadata = gemini_response.get("usageMetadata", {})
        usage = {
            "prompt_tokens": usage_metadata.get("promptTokenCount", 0),
            "completion_tokens": usage_metadata.get("candidatesTokenCount", 0),
            "total_tokens": usage_metadata.get("totalTokenCount", 0)
        }

        # æ˜ å°„Geminiçš„finishReasonåˆ°AutoGenæœŸæœ›çš„å€¼
        finish_reason_map = {
            "STOP": "stop",
            "MAX_TOKENS": "length",
            "SAFETY": "content_filter",
            "RECITATION": "content_filter",
            "OTHER": "unknown"
        }

        gemini_finish_reason = candidate.get("finishReason", "STOP")
        finish_reason = finish_reason_map.get(gemini_finish_reason, "unknown")

        # å¦‚æœæœ‰å‡½æ•°è°ƒç”¨ï¼Œfinish_reasonåº”è¯¥æ˜¯function_calls
        if function_calls:
            finish_reason = "function_calls"

        # æ ¹æ®CreateResultçš„æœŸæœ›æ ¼å¼ï¼Œcontentåº”è¯¥æ˜¯å­—ç¬¦ä¸²æˆ–FunctionCallåˆ—è¡¨
        if function_calls:
            # å¦‚æœæœ‰å‡½æ•°è°ƒç”¨ï¼Œè¿”å›å‡½æ•°è°ƒç”¨åˆ—è¡¨
            result_content = function_calls
        else:
            # å¦‚æœåªæœ‰æ–‡æœ¬ï¼Œè¿”å›æ–‡æœ¬å†…å®¹
            result_content = message_content if message_content else "No response"

        # è¿”å›CreateResult
        return CreateResult(
            finish_reason=finish_reason,
            content=result_content,
            usage=usage,
            cached=False,
            logprobs=None
        )

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.client.aclose()

# å…¨å±€å®¢æˆ·ç«¯ç¼“å­˜
_clients = {}

def get_native_gemini_client(model: str = "gemini-2.5-pro") -> NativeGeminiClient:
    """
    è·å–åŸç”ŸGeminiå®¢æˆ·ç«¯å®ä¾‹
    """
    if model not in _clients:
        _clients[model] = NativeGeminiClient(model=model)
        logger.info(f"âœ… Created new native Gemini client for model: {model}")

    return _clients[model]